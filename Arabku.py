# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ouojIF5TIUw_wdWEd8EP92RdYlRRCGXd

# **Install Tensorflow**
"""

!pip install tensorflow

"""# **Preprocess**"""

from google.colab import drive
drive.mount('/content/drive')

import keras
from keras.models import Sequential
from keras.models import load_model
from keras.layers import Dense, Conv2D, MaxPooling2D , Flatten
from keras.preprocessing.image import ImageDataGenerator

import shutil
from shutil import copyfile

import random
import numpy as np
import os
import matplotlib.pyplot as plt

base_dir = '/content/drive/Shareddrives/Capstone'

if os.path.exists(f'{base_dir}/datasets') == True:

  !rm -rf '/content/drive/Shareddrives/Capstone/datasets'

else:
  os.mkdir(f'{base_dir}/datasets')

  os.mkdir(f'{base_dir}/datasets/train')

  os.mkdir(f'{base_dir}/datasets/train/Kursiyun')
  os.mkdir(f'{base_dir}/datasets/train/Maktabun')
  os.mkdir(f'{base_dir}/datasets/train/Qolamun')
  os.mkdir(f'{base_dir}/datasets/train/Saatun')
  os.mkdir(f'{base_dir}/datasets/train/Sabburotun')

  os.mkdir(f'{base_dir}/datasets/test')
  os.mkdir(f'{base_dir}/datasets/test/Kursiyun')
  os.mkdir(f'{base_dir}/datasets/test/Maktabun')
  os.mkdir(f'{base_dir}/datasets/test/Qolamun')
  os.mkdir(f'{base_dir}/datasets/test/Saatun')
  os.mkdir(f'{base_dir}/datasets/test/Sabburotun')

def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):

  files=[]
  for filename in os.listdir(SOURCE):
    file = SOURCE+filename
    if os.path.getsize(file) > 0:
      files.append(filename)
    else:
      print('File is empty')

    training_length = int(len(files)*SPLIT_SIZE)
    testing_length = int(len(files)-training_length)
    shuffled_set = random.sample(files, len(files))
    training_set = shuffled_set[0:training_length]
    testing_set = shuffled_set[-testing_length:]

  for filename in training_set:
    source_file = SOURCE+filename
    destination_file = TRAINING+filename
    copyfile(source_file, destination_file)

  for filename in testing_set:
    source_file = SOURCE+filename
    destination_file = TESTING+filename
    copyfile(source_file,destination_file)

Kursiyun_SOURCE_DIR = f'{base_dir}/Capstone/Kursiyun/'
Maktabun_SOURCE_DIR = f'{base_dir}/Capstone/Maktabun/'
Qolamun_SOURCE_DIR = f'{base_dir}/Capstone/Qolamun/'
Saatun_SOURCE_DIR = f'{base_dir}/Capstone/Saatun/'
Sabburotun_SOURCE_DIR = f'{base_dir}/Capstone/Sabburotun/'

Kursiyun_TRAINING_DIR = f'{base_dir}/datasets/train/Kursiyun/'
Maktabun_TRAINING_DIR = f'{base_dir}/datasets/train/Maktabun/'
Qolamun_TRAINING_DIR = f'{base_dir}/datasets/train/Qolamun/'
Saatun_TRAINING_DIR = f'{base_dir}/datasets/train/Saatun/'
Sabburotun_TRAINING_DIR = f'{base_dir}/datasets/train/Sabburotun/'

Kursiyun_TESTING_DIR = f'{base_dir}/datasets/test/Kursiyun/'
Maktabun_TESTING_DIR = f'{base_dir}/datasets/test/Maktabun/'
Qolamun_TESTING_DIR = f'{base_dir}/datasets/test/Qolamun/'
Saatun_TESTING_DIR = f'{base_dir}/datasets/test/Saatun/'
Sabburotun_TESTING_DIR = f'{base_dir}/datasets/test/Sabburotun/'

split_size = 0.8

split_data(Kursiyun_SOURCE_DIR, Kursiyun_TRAINING_DIR, Kursiyun_TESTING_DIR, split_size)
split_data(Maktabun_SOURCE_DIR, Maktabun_TRAINING_DIR, Maktabun_TESTING_DIR, split_size)
split_data(Qolamun_SOURCE_DIR, Qolamun_TRAINING_DIR, Qolamun_TESTING_DIR, split_size)
split_data(Saatun_SOURCE_DIR, Saatun_TRAINING_DIR, Saatun_TESTING_DIR, split_size)
split_data(Sabburotun_SOURCE_DIR, Sabburotun_TRAINING_DIR, Sabburotun_TESTING_DIR, split_size)

"""# **Model**"""

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150,150,3)))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(5, activation='softmax'))

from keras.optimizers import Adam
opt = Adam(learning_rate=0.0001)
model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])

model.summary()

"""# **Train**"""

TRAINING_DIR = f'{base_dir}/datasets/train'
train_datagen = ImageDataGenerator(rescale=1.0/255.)
train_generator = train_datagen.flow_from_directory(TRAINING_DIR,
                                                         batch_size=64,
                                                         class_mode='categorical',
                                                         target_size=(150,150))

VALIDATION_DIR = f'{base_dir}/datasets/test'
validation_datagen = ImageDataGenerator(rescale=1.0/255.)
validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,
                                                         batch_size=64,
                                                         class_mode='categorical',
                                                         target_size=(150,150))

history = model.fit(train_generator,
              epochs=30,
              verbose=1,
              validation_data=validation_generator)

model.evaluate(validation_generator)

accuracy=history.history['accuracy']
validation_accuracy=history.history['val_accuracy']
loss=history.history['loss']
validation_loss=history.history['val_loss']

epochs = range(len(accuracy))

plt.plot(epochs, accuracy, 'g', label='Training accuracy')
plt.plot(epochs, validation_accuracy, 'r', label='Validation accuracy')
plt.title('Training accuracy and validation accuracy')
plt.legend(loc=0)
plt.show()

plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, validation_loss, 'r', label='Validation loss')
plt.title('Training loss and validation loss')
plt.legend(loc=0)
plt.show()

import keras.utils as image
from google.colab import files

class_labels = ['Kursiyun', 'Maktabun', 'Qolamun', 'Saatun', 'Sabburotun']

def preprocess_image(img_path):
    img = image.load_img(img_path, target_size=(150, 150))
    imgplot = plt.imshow(img)
    plt.show()
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = img / 255.0
    return img

def predict_image(img_path):
    img = preprocess_image(img_path)
    prediction = model.predict(img)
    predicted_class = np.argmax(prediction)
    predicted_label = class_labels[predicted_class]
    percentage = prediction[0][predicted_class] * 100
    print('Objek adalah :', predicted_label)
    txt = 'Persentase: {:.2f}%'
    print(txt.format(percentage))

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

uploaded = files.upload()

for fn in uploaded.keys():
    img_path = '/content/' + fn
    predict_image(img_path)

"""# **SAVE MODEL**"""

model.save('object_classification_ArabKu.h5')
print('Model Saved!')

"""# **LOAD MODEL**"""

savedModel=load_model('object_classification_ArabKu.h5')
savedModel.summary()

"""# **Convert to Tensorflow Lite**"""

import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open("model.tflite", 'wb') as f:
  f.write(tflite_model)